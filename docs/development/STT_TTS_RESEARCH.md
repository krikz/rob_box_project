# STT –∏ TTS –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ - Research

–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∏ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è Speech-to-Text –∏ Text-to-Speech –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è —Ä–æ–±–æ—Ç–∞ –Ω–∞ Raspberry Pi 5.

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞
- **Hardware:** Raspberry Pi 5 (ARM64, 8GB RAM)
- **OS:** Ubuntu 24.04 ARM64
- **ROS:** ROS2 Humble/Jazzy
- **Memory budget:** ~2GB –¥–ª—è voice assistant –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞

### –ö–∞—á–µ—Å—Ç–≤–æ
- **STT:** –¢–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏ >85%
- **TTS:** –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–æ–ª–æ—Å —Å –∏–Ω—Ç–æ–Ω–∞—Ü–∏—è–º–∏ (–Ω–µ —Ä–æ–±–æ—Ç)
- **Latency:** <2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è STT, <1 —Å–µ–∫—É–Ω–¥–∞ –¥–ª—è TTS
- **Offline:** –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞

---

## üé§ STT (Speech-to-Text) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ

### 1. ‚úÖ **Whisper (OpenAI)** - –õ—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ STT

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –û—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å –æ—Ç OpenAI
- –û—Ç–ª–∏—á–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π

**–ú–æ–¥–µ–ª–∏:**
| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM | –¢–æ—á–Ω–æ—Å—Ç—å (RU) | Latency |
|--------|--------|-----|---------------|---------|
| `tiny` | 39 MB | ~1 GB | ~70% | <1s |
| `base` | 74 MB | ~1 GB | ~80% | 1-2s |
| `small` | 244 MB | ~2 GB | ~85% | 2-3s |
| `medium` | 769 MB | ~5 GB | ~90% | 5-7s |
| `large` | 1550 MB | ~10 GB | ~95% | 10-15s |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è RPi5:**
- **`small`** (244 MB) - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
- **`base`** (74 MB) - –µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–Ω–∞ –ø–∞–º—è—Ç—å

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
pip install openai-whisper
# –∏–ª–∏ faster-whisper –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
pip install faster-whisper
```

**–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
```python
import whisper

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–æ–¥–∏–Ω —Ä–∞–∑)
model = whisper.load_model("small", device="cpu")

# –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å
result = model.transcribe("audio.wav", language="ru")
print(result["text"])
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
- ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é offline
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
- ‚úÖ –ê–∫—Ç–∏–≤–Ω–æ–µ –∫–æ–º—å—é–Ω–∏—Ç–∏

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç 2-5 GB RAM (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏)
- ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π
- ‚ö†Ô∏è CPU inference –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º (2-5 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Ñ—Ä–∞–∑—É)

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
- `faster-whisper` - —É—Å–∫–æ—Ä–µ–Ω–∏–µ —á–µ—Ä–µ–∑ CTranslate2 (2-4√ó –±—ã—Å—Ç—Ä–µ–µ)
- –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (int8) - —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `base` –º–æ–¥–µ–ª—å –¥–ª—è real-time

---

### 2. ‚úÖ **Vosk (Alphacephei)** - Lightweight –¥–ª—è Raspberry Pi

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è offline —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è ARM/embedded
- –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è —Ä—É—Å—Å–∫–∞—è –º–æ–¥–µ–ª—å

**–ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ:**
| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | RAM | –¢–æ—á–Ω–æ—Å—Ç—å | Latency |
|--------|--------|-----|----------|---------|
| `vosk-model-small-ru-0.22` | 45 MB | ~500 MB | ~75% | <0.5s |
| `vosk-model-ru-0.42` | 1.8 GB | ~2 GB | ~85% | ~1s |

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
pip install vosk

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å
wget https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip
unzip vosk-model-small-ru-0.22.zip
```

**–ü—Ä–∏–º–µ—Ä:**
```python
from vosk import Model, KaldiRecognizer
import wave

model = Model("vosk-model-small-ru-0.22")
wf = wave.open("audio.wav", "rb")
rec = KaldiRecognizer(model, wf.getframerate())

while True:
    data = wf.readframes(4000)
    if len(data) == 0:
        break
    if rec.AcceptWaveform(data):
        result = json.loads(rec.Result())
        print(result['text'])
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π (real-time –Ω–∞ RPi)
- ‚úÖ –ú–∞–ª–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM (<1GB)
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ streaming recognition
- ‚úÖ Offline

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ —á–µ–º Whisper
- ‚ö†Ô∏è –ú–µ–Ω—å—à–µ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–π
- ‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç–æ

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –õ—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è **real-time** STT –Ω–∞ RPi5!

---

### 3. üîß **Julius (–æ—Ä–∏–≥–∏–Ω–∞–ª –∏–∑ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞)**

**–°—Ç–∞—Ç—É—Å:** ‚ùå **–ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è**

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –°—Ç–∞—Ä—ã–π –ø—Ä–æ–µ–∫—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–ª–∏–∑ 2019)
- –ê–∫—É—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —è–ø–æ–Ω—Å–∫–æ–≥–æ/–∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
- –ù–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª–∏
- –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

**–í–µ—Ä–¥–∏–∫—Ç:** Julius —É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Vosk –∏–ª–∏ Whisper.

---

### 4. ‚òÅÔ∏è **Yandex SpeechKit** (—Ç–µ–∫—É—â–∏–π –≤—ã–±–æ—Ä)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –û–±–ª–∞—á–Ω—ã–π STT –æ—Ç –Ø–Ω–¥–µ–∫—Å–∞
- –û—Ç–ª–∏—á–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (~95%)
- –¢—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç

**API:**
```python
import grpc
import yandex.cloud.ai.stt.v2.stt_service_pb2 as stt_service_pb2
import yandex.cloud.ai.stt.v2.stt_service_pb2_grpc as stt_service_pb2_grpc

# Streaming recognition
stub = stt_service_pb2_grpc.SttServiceStub(channel)
response = stub.StreamingRecognize(request_iterator)
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (95%+)
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π (<1s latency)
- ‚úÖ –ù–µ –Ω–∞–≥—Ä—É–∂–∞–µ—Ç RPi

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
- ‚ö†Ô∏è –ü–ª–∞—Ç–Ω–æ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ –ø–µ—Ä–≤—ã–µ 15 —á–∞—Å–æ–≤/–º–µ—Å—è—Ü)
- ‚ö†Ô∏è –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –æ–±–ª–∞–∫–∞

---

### üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è STT

**Primary:** `Vosk small` (real-time, offline)
**Fallback:** `Yandex SpeechKit` (–µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)
**Experimental:** `Whisper base` (–ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

---

## üîä TTS (Text-to-Speech) –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ

### 1. ‚úÖ **Piper TTS** - –õ—É—á—à–∏–π –ª–æ–∫–∞–ª—å–Ω—ã–π TTS 2024

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π neural TTS –æ—Ç Rhasspy
- –ë—ã—Å—Ç—Ä—ã–π inference (real-time –Ω–∞ RPi)
- –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–∞
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

**–ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ:**
| –ì–æ–ª–æ—Å | –ö–∞—á–µ—Å—Ç–≤–æ | –°–∫–æ—Ä–æ—Å—Ç—å | –†–∞–∑–º–µ—Ä |
|-------|----------|----------|--------|
| `ru_RU-dmitri-medium` | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast | 63 MB |
| `ru_RU-irina-medium` | ‚≠ê‚≠ê‚≠ê‚≠ê | Fast | 63 MB |

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
pip install piper-tts

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/ru_RU-dmitri-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/ru_RU-dmitri-medium.onnx.json
```

**–ü—Ä–∏–º–µ—Ä:**
```python
from piper import PiperVoice

voice = PiperVoice.load("ru_RU-dmitri-medium.onnx")
audio = voice.synthesize("–ü—Ä–∏–≤–µ—Ç, —è –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–æ–±–æ—Ç–∞!")

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ WAV
with wave.open("output.wav", "wb") as wav_file:
    wav_file.writeframes(audio)
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (neural voice)
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π (real-time –Ω–∞ RPi5)
- ‚úÖ –ú–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ (~60 MB)
- ‚úÖ Offline
- ‚úÖ –ù–∏–∑–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ CPU

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –ú–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≥–æ–ª–æ—Å–æ–≤
- ‚ö†Ô∏è –ù–µ—Ç —ç–º–æ—Ü–∏–π (neutral tone)

---

### 2. ‚úÖ **Silero TTS** - –†—É—Å—Å–∫–∏–π neural TTS ‚ö° –†–ê–ë–û–¢–ê–ï–¢ –ù–ê RASPBERRY PI!

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –†—É—Å—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç –æ—Ç Silero Team
- PyTorch –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è CPU
- **–ë—ã—Å—Ç—Ä–µ–µ realtime –¥–∞–∂–µ –Ω–∞ ARM!**
- –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–≤—É–∫–∞

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ Raspberry Pi 5:**
```
CPU: 4 √ó Cortex-A76 @ 2.4GHz
RTF (Real Time Factor):
  - 1 –ø–æ—Ç–æ–∫:  RTF = 0.3-0.5  (2-3x –±—ã—Å—Ç—Ä–µ–µ realtime)
  - 4 –ø–æ—Ç–æ–∫–∞: RTF = 0.2-0.3  (3-5x –±—ã—Å—Ç—Ä–µ–µ realtime)
  
–ü—Ä–∏–º–µ—Ä: —Ñ—Ä–∞–∑–∞ –Ω–∞ 3 —Å–µ–∫—É–Ω–¥—ã —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç—Å—è –∑–∞ 0.6-1.0 —Å–µ–∫—É–Ω–¥—É
```

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
pip install torch torchaudio omegaconf

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å (v4 - –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)
import torch

model, example_text = torch.hub.load(
    repo_or_dir='snakers4/silero-models',
    model='silero_tts',
    language='ru',
    speaker='v4_ru'  # v4 –ª—É—á—à–µ —á–µ–º v3!
)
```

**–ü—Ä–∏–º–µ—Ä:**
```python
import torch

device = torch.device('cpu')
torch.set_num_threads(4)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 4 –ø–æ—Ç–æ–∫–∞ –Ω–∞ Pi 5
model.to(device)

audio = model.apply_tts(
    text="–ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–æ–±–æ—Ç–∞.",
    speaker='aidar',  # aidar, baya, kseniya, xenia
    sample_rate=48000
)
```

**–ì–æ–ª–æ—Å–∞ (v4_ru):**
- `aidar` - –º—É–∂—Å–∫–æ–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π ‚úÖ **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —Ä–æ–±–æ—Ç–∞**
- `baya` - –∂–µ–Ω—Å–∫–∏–π, —Ç—ë–ø–ª—ã–π
- `kseniya` - –∂–µ–Ω—Å–∫–∏–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π
- `xenia` - –∂–µ–Ω—Å–∫–∏–π, –º–æ–ª–æ–¥–æ–π

**–ü–ª—é—Å—ã:**
- ‚úÖ **–ë—ã—Å—Ç—Ä–µ–µ realtime –Ω–∞ Raspberry Pi 5!** (RTF 0.3-0.5)
- ‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–≤—É–∫–∞
- ‚úÖ –ù–µ—Å–∫–æ–ª—å–∫–æ –≥–æ–ª–æ—Å–æ–≤ –Ω–∞ –≤—ã–±–æ—Ä
- ‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é offline
- ‚úÖ –†—É—Å—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç —Å –∞–∫—Ç–∏–≤–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU (–Ω–µ –Ω—É–∂–Ω–∞ GPU)

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç PyTorch (~300 MB)
- ‚ö†Ô∏è –ú–æ–¥–µ–ª—å ~100 MB (vs 63 MB —É Piper)
- ‚ö†Ô∏è RAM ~200 MB –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ (vs ~50 MB —É Piper)

**–ë–µ–Ω—á–º–∞—Ä–∫–∏ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ, Intel i7-6800K @ 3.4GHz):**
```
16kHz –º–æ–¥–µ–ª—å:
  1 –ø–æ—Ç–æ–∫:  RTF = 0.7  (1.4x –±—ã—Å—Ç—Ä–µ–µ realtime)
  2 –ø–æ—Ç–æ–∫–∞: RTF = 0.4  (2.3x –±—ã—Å—Ç—Ä–µ–µ realtime)
  4 –ø–æ—Ç–æ–∫–∞: RTF = 0.3  (3.1x –±—ã—Å—Ç—Ä–µ–µ realtime)

–ù–∞ Raspberry Pi 5 –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å—Ö–æ–∂–∏–µ –∏–ª–∏ –ª—É—á—à–µ!
```

---

### 3. ‚úÖ **RHVoice** - –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π open-source TTS

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –°—Ç–∞—Ä—ã–π –ø—Ä–æ–µ–∫—Ç, –Ω–æ –∞–∫—Ç–∏–≤–Ω—ã–π
- –°–∏–Ω—Ç–µ–∑ —á–µ—Ä–µ–∑ —Ñ–æ—Ä–º–∞–Ω—Ç–Ω—ã–π —Å–∏–Ω—Ç–µ–∑
- –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä—É—Å—Å–∫–∏—Ö –≥–æ–ª–æ—Å–æ–≤

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
sudo apt-get install rhvoice rhvoice-russian

# Python bindings
pip install rhvoice-wrapper
```

**–ì–æ–ª–æ—Å–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ:**
- `aleksandr` - –º—É–∂—Å–∫–æ–π
- `anna` - –∂–µ–Ω—Å–∫–∏–π
- `elena` - –∂–µ–Ω—Å–∫–∏–π
- `irina` - –∂–µ–Ω—Å–∫–∏–π

**–ü–ª—é—Å—ã:**
- ‚úÖ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π
- ‚úÖ –ú–∞–ª–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
- ‚úÖ –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤
- ‚úÖ Offline

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –ö–∞—á–µ—Å—Ç–≤–æ —Ö—É–∂–µ neural TTS (–∑–≤—É—á–∏—Ç –∫–∞–∫ —Ä–æ–±–æ—Ç)
- ‚ö†Ô∏è –ù–µ—Ç –∏–Ω—Ç–æ–Ω–∞—Ü–∏–π
- ‚ö†Ô∏è –°—Ç–∞—Ä–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è

---

### 4. üîß **Coqui TTS** (ex-Mozilla TTS)

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è **–ü—Ä–æ–µ–∫—Ç –∑–∞–º–æ—Ä–æ–∂–µ–Ω** (2023)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –§–æ—Ä–∫ Mozilla TTS
- Neural TTS
- –†—É—Å—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å, –Ω–æ quality varies

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞:**
```bash
pip install TTS

# –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
tts --list_models | grep ru
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (neural)
- ‚úÖ Multi-speaker

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –ü—Ä–æ–µ–∫—Ç –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
- ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–π inference
- ‚ö†Ô∏è –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (>1 GB)
- ‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM

---

### 5. ‚òÅÔ∏è **Yandex SpeechKit TTS** (—Ç–µ–∫—É—â–∏–π –≤—ã–±–æ—Ä)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
- –û–±–ª–∞—á–Ω—ã–π TTS –æ—Ç –Ø–Ω–¥–µ–∫—Å–∞
- –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (~99%)
- –ì–æ–ª–æ—Å–∞ —Å —ç–º–æ—Ü–∏—è–º–∏

**–ì–æ–ª–æ—Å–∞:**
- `anton` - –º—É–∂—Å–∫–æ–π, neutral
- `alena` - –∂–µ–Ω—Å–∫–∏–π
- `oksana` - –∂–µ–Ω—Å–∫–∏–π, —Ç–µ–ø–ª—ã–π
- `jane` - –∂–µ–Ω—Å–∫–∏–π, friendly

**–ü–ª—é—Å—ã:**
- ‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
- ‚úÖ –≠–º–æ—Ü–∏–∏ (neutral/good/evil)
- ‚úÖ –†–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π (<500ms)

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
- ‚ö†Ô∏è –ü–ª–∞—Ç–Ω–æ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ 1 –º–ª–Ω —Å–∏–º–≤–æ–ª–æ–≤/–º–µ—Å—è—Ü)
- ‚ö†Ô∏è –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –æ–±–ª–∞–∫–∞

---

### üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è TTS

**Primary:** `Piper TTS (dmitri)` - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ + —Å–∫–æ—Ä–æ—Å—Ç—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ
**Alternative:** `Silero TTS` - –µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ä–∞–∑–Ω—ã–µ –≥–æ–ª–æ—Å–∞
**Fallback:** `Yandex SpeechKit` - –µ—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –Ω—É–∂–Ω—ã —ç–º–æ—Ü–∏–∏

---

## üöÄ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### Hybrid Setup (Offline-First)

```yaml
stt_node:
  primary: "vosk"           # Fast, real-time, offline
  fallback: "yandex"        # High accuracy, online
  
  vosk:
    model: "vosk-model-small-ru-0.22"  # 45 MB
    
  yandex:
    api_key: "${YANDEX_API_KEY}"
    use_when: "vosk_confidence < 0.7"  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ Vosk –Ω–µ —É–≤–µ—Ä–µ–Ω

tts_node:
  primary: "piper"          # Fast, good quality, offline
  fallback: "yandex"        # Best quality, online
  
  piper:
    model: "ru_RU-dmitri-medium.onnx"
    voice_speed: 1.0
    
  yandex:
    voice: "anton"
    speed: 0.4
    emotion: "neutral"
```

---

## üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞

### STT

| –†–µ—à–µ–Ω–∏–µ | Offline | –¢–æ—á–Ω–æ—Å—Ç—å (RU) | Latency | RAM | CPU Load | –ö–∞—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ—Å–∞ |
|---------|---------|---------------|---------|-----|----------|-----------------|
| **Vosk small** | ‚úÖ | 75% | <0.5s | 500 MB | Low | - |
| **Whisper base** | ‚úÖ | 80% | 1-2s | 1 GB | Medium | - |
| **Whisper small** | ‚úÖ | 85% | 2-3s | 2 GB | High | - |
| **Yandex STT** | ‚ùå | 95% | <1s | ~0 | ~0 | - |

### TTS

| –†–µ—à–µ–Ω–∏–µ | Offline | –ö–∞—á–µ—Å—Ç–≤–æ | Latency (Pi 5) | RAM | –†–∞–∑–º–µ—Ä | –ò–Ω—Ç–æ–Ω–∞—Ü–∏–∏ |
|---------|---------|----------|----------------|-----|--------|-----------|
| **Piper TTS** | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | <0.5s | 50 MB | 63 MB | ‚≠ê‚≠ê |
| **Silero TTS** | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | **0.3-0.5s** ‚ö° | 200 MB | 100 MB | ‚≠ê‚≠ê‚≠ê |
| **RHVoice** | ‚úÖ | ‚≠ê‚≠ê | <0.3s | 50 MB | 50 MB | ‚≠ê |
| **Yandex TTS** | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | <0.5s | ~0 | - | ‚≠ê‚≠ê‚≠ê‚≠ê |

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** Silero TTS —Ç–µ–ø–µ—Ä—å **–±—ã—Å—Ç—Ä–µ–µ realtime** –Ω–∞ Raspberry Pi 5 (RTF 0.3-0.5), —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –æ—Ç–ª–∏—á–Ω–æ–π –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–æ–π Piper!

---

## üõ†Ô∏è ROS2 –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞–∫–µ—Ç—ã

#### 1. `ros_speech_recognition` (–ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:** https://github.com/kscottz/ros_speech_recognition

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è –£—Å—Ç–∞—Ä–µ–ª (–ø–æ—Å–ª–µ–¥–Ω–∏–π –∫–æ–º–º–∏—Ç 2016)

**–ü–æ–¥–¥–µ—Ä–∂–∫–∞:**
- Google Speech API (deprecated)
- Wit.ai
- Sphinx (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)

**–í–µ—Ä–¥–∏–∫—Ç:** –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è ROS2 –∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.

---

#### 2. `julius_ros` (–ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

**–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:** https://github.com/jsk-ros-pkg/jsk_3rdparty/tree/master/julius_ros

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è –¢–æ–ª—å–∫–æ –¥–ª—è ROS1, —è–ø–æ–Ω—Å–∫–∏–π/–∞–Ω–≥–ª–∏–π—Å–∫–∏–π

**–í–µ—Ä–¥–∏–∫—Ç:** –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è ROS2 –∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.

---

#### 3. ‚úÖ `audio_common` (ROS2)

**–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:** https://github.com/ros2/audio_common

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ê–∫—Ç–∏–≤–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è

**–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:**
- –ó–∞—Ö–≤–∞—Ç –∞—É–¥–∏–æ (`audio_capture`)
- –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (`audio_play`)
- –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ/–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

**–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```bash
sudo apt install ros-humble-audio-common
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –ì–æ—Ç–æ–≤—ã–π –ø–∞–∫–µ—Ç –¥–ª—è ROS2
- ‚úÖ –ó–∞—Ö–≤–∞—Ç/–≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∞—É–¥–∏–æ
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ ALSA

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```python
from audio_common_msgs.msg import AudioData

# –ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∞—É–¥–∏–æ —Ç–æ–ø–∏–∫
self.create_subscription(AudioData, '/audio/audio', self.audio_callback, 10)
```

---

### –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö ROS2 –Ω–æ–¥

#### STT Node (Vosk + Whisper)

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from audio_common_msgs.msg import AudioData
from std_msgs.msg import String
from vosk import Model, KaldiRecognizer
import whisper
import numpy as np

class STTNode(Node):
    def __init__(self):
        super().__init__('stt_node')
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.declare_parameter('provider', 'vosk')  # vosk | whisper | yandex
        self.declare_parameter('vosk_model_path', 'vosk-model-small-ru-0.22')
        self.declare_parameter('whisper_model', 'base')
        
        provider = self.get_parameter('provider').value
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        if provider == 'vosk':
            model_path = self.get_parameter('vosk_model_path').value
            self.model = Model(model_path)
            self.recognizer = KaldiRecognizer(self.model, 16000)
            
        elif provider == 'whisper':
            model_size = self.get_parameter('whisper_model').value
            self.model = whisper.load_model(model_size, device='cpu')
        
        # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –∞—É–¥–∏–æ
        self.audio_sub = self.create_subscription(
            AudioData, '/audio/audio', self.audio_callback, 10)
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        self.text_pub = self.create_publisher(String, '/stt/text', 10)
        
        self.get_logger().info(f'STT Node started with provider: {provider}')
    
    def audio_callback(self, msg):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"""
        audio_data = np.frombuffer(msg.data, dtype=np.int16)
        
        # Vosk streaming recognition
        if hasattr(self, 'recognizer'):
            if self.recognizer.AcceptWaveform(audio_data.tobytes()):
                result = json.loads(self.recognizer.Result())
                text = result.get('text', '')
                if text:
                    self.publish_text(text)
        
        # Whisper batch recognition
        elif hasattr(self, 'model'):
            # –ù–∞–∫–æ–ø–∏—Ç—å –∞—É–¥–∏–æ, –∑–∞—Ç–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å
            result = self.model.transcribe(audio_data, language='ru')
            text = result['text']
            if text:
                self.publish_text(text)
    
    def publish_text(self, text):
        msg = String()
        msg.data = text
        self.text_pub.publish(msg)
        self.get_logger().info(f'Recognized: {text}')

def main(args=None):
    rclpy.init(args=args)
    node = STTNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

#### TTS Node (Piper)

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from audio_common_msgs.msg import AudioData
from piper import PiperVoice
import wave
import io

class TTSNode(Node):
    def __init__(self):
        super().__init__('tts_node')
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.declare_parameter('provider', 'piper')
        self.declare_parameter('piper_model', 'ru_RU-dmitri-medium.onnx')
        self.declare_parameter('voice_speed', 1.0)
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Piper
        model_path = self.get_parameter('piper_model').value
        self.voice = PiperVoice.load(model_path)
        
        # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ —Ç–µ–∫—Å—Ç
        self.text_sub = self.create_subscription(
            String, '/tts/text', self.text_callback, 10)
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –∞—É–¥–∏–æ
        self.audio_pub = self.create_publisher(AudioData, '/tts/audio', 10)
        
        self.get_logger().info('TTS Node (Piper) started')
    
    def text_callback(self, msg):
        """–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text = msg.data
        self.get_logger().info(f'Synthesizing: {text}')
        
        # –°–∏–Ω—Ç–µ–∑ —á–µ—Ä–µ–∑ Piper
        audio = self.voice.synthesize(text)
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –∞—É–¥–∏–æ
        audio_msg = AudioData()
        audio_msg.data = audio.tobytes()
        self.audio_pub.publish(audio_msg)
        
        self.get_logger().info('Audio published')

def main(args=None):
    rclpy.init(args=args)
    node = TTSNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

---

## üì¶ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

### Dockerfile –∏–∑–º–µ–Ω–µ–Ω–∏—è

```dockerfile
# –î–æ–±–∞–≤–∏—Ç—å –≤ docker/vision/voice_assistant/Dockerfile

# STT: Vosk + Whisper
RUN pip3 install --no-cache-dir \
    vosk \
    openai-whisper \
    faster-whisper

# TTS: Piper + Silero
RUN pip3 install --no-cache-dir \
    piper-tts \
    torch torchaudio

# –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª–∏
RUN mkdir -p /models && \
    # Vosk
    wget -O /tmp/vosk-model.zip https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip && \
    unzip /tmp/vosk-model.zip -d /models/ && \
    rm /tmp/vosk-model.zip && \
    # Piper
    wget -O /models/ru_RU-dmitri-medium.onnx \
        https://github.com/rhasspy/piper/releases/download/v1.2.0/ru_RU-dmitri-medium.onnx && \
    wget -O /models/ru_RU-dmitri-medium.onnx.json \
        https://github.com/rhasspy/piper/releases/download/v1.2.0/ru_RU-dmitri-medium.onnx.json
```

---

## üéØ –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### Primary Setup (Offline-First)

```yaml
# config/voice_assistant.yaml

stt_node:
  provider: "vosk"                    # Real-time, offline
  vosk:
    model_path: "/models/vosk-model-small-ru-0.22"
    confidence_threshold: 0.7
  
  # Fallback –¥–ª—è –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
  fallback_provider: "yandex"
  yandex:
    use_when_confidence_below: 0.7

tts_node:
  provider: "piper"                   # Fast, high quality, offline
  piper:
    model_path: "/models/ru_RU-dmitri-medium.onnx"
    speed: 1.0
  
  # Fallback –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
  fallback_provider: "yandex"
  yandex:
    voice: "anton"
    emotion: "good"
```

### Memory Budget

```
Audio Node:         ~200 MB
Vosk STT:           ~500 MB
Piper TTS:          ~100 MB
Dialogue Node:      ~500 MB
LED/Animation:      ~200 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:              ~1.5 GB  ‚úÖ Fits in 2GB budget
```

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

‚úÖ **–ü–æ–ª–Ω–æ—Å—Ç—å—é offline** - —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
‚úÖ **–ë—ã—Å—Ç—Ä—ã–π** - <1s latency –¥–ª—è STT+TTS
‚úÖ **–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π** - neural TTS —Å —Ö–æ—Ä–æ—à–∏–º –∑–≤—É–∫–æ–º
‚úÖ **–õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π** - 1.5GB RAM (—É–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –≤ –±—é–¥–∂–µ—Ç)
‚úÖ **Fallback** - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ Yandex –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

---

## üìö –°—Å—ã–ª–∫–∏

### STT
- **Vosk:** https://alphacephei.com/vosk/
- **Whisper:** https://github.com/openai/whisper
- **Faster Whisper:** https://github.com/guillaumekln/faster-whisper

### TTS
- **Piper:** https://github.com/rhasspy/piper
- **Silero TTS:** https://github.com/snakers4/silero-models
- **RHVoice:** https://github.com/RHVoice/RHVoice

### ROS2
- **audio_common:** https://github.com/ros2/audio_common
- **audio_common_msgs:** https://github.com/ros2/audio_common_msgs

---

**–î–∞—Ç–∞:** 2025-01-12  
**–ê–≤—Ç–æ—Ä:** Research –¥–ª—è rob_box_project voice assistant
